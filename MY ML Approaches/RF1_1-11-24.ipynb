{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"B6Y4fFQANMjS","outputId":"b2cea1b7-c26c-4acf-df04-04bf6e0806e9"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'encoded_file.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 1: Determine the total number of rows and skip the last row\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoded_file.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Ignore the last row\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the phenotype vector (DTF column from the 2nd column in Phenotype_DTF_2019.csv)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m phenotype_vector \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhenotype_DTF_2019.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\n","File \u001b[0;32m~/anaconda3/envs/PR3_env1/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'encoded_file.csv'"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","\n","# Step 1: Determine the total number of rows and skip the last row\n","total_rows = sum(1 for row in open('encoded_file.csv')) - 1  # Ignore the last row\n","\n","# Load the phenotype vector (DTF column from the 2nd column in Phenotype_DTF_2019.csv)\n","phenotype_vector = pd.read_csv(\"Phenotype_DTF_2019.csv\").iloc[:, 1]\n","\n","# Initialize lists to store results\n","performance_scores = []\n","chromosomes = []\n","positions = []\n","processed_snps = []  # To keep track of successfully processed SNP names\n","\n","# Define the scaler once\n","scaler = StandardScaler()\n","\n","# Step 2: Read the SNP file in chunks (10 rows at a time, excluding the last row)\n","chunk_size = 10  # Fixed chunk size of 10 rows\n","snp_reader = pd.read_csv('encoded_file.csv', chunksize=chunk_size, iterator=True)\n","\n","# Initialize the number of processed rows\n","processed_rows = 0\n","\n","# Step 3: Process 10 rows in each chunk, and iterate over SNP columns one by one\n","for chunk in snp_reader:\n","    # If the last chunk goes beyond the row limit, trim it to avoid processing the last row\n","    if processed_rows + len(chunk) > total_rows:\n","        chunk = chunk.iloc[:total_rows - processed_rows, :]\n","\n","    # Drop the first index column (assumed to be the index) and iterate over SNP columns\n","    for snp in chunk.columns[1:]:\n","        # Extract chromosome and position from SNP name (assuming 'chromosome_position' format)\n","        chrom, pos = snp.split('_')\n","        chrom = int(chrom[1:])  # Assumes SNP names are like 'C1_1234'\n","        pos = int(pos)\n","\n","        # Extract the SNP column for this chunk, reshape, and scale the SNP data\n","        X_snp = chunk[snp].values.reshape(-1, 1)  # SNP data from the chunk (up to 10 rows)\n","        X_snp_scaled = scaler.fit_transform(X_snp)\n","\n","        # Ensure the slice of the phenotype vector matches the number of rows in the SNP data\n","        phenotype_chunk = phenotype_vector.iloc[processed_rows:processed_rows + len(X_snp_scaled)].values\n","\n","        # Check if lengths match; if not, skip this SNP column\n","        if len(X_snp_scaled) != len(phenotype_chunk):\n","            print(f\"Skipping SNP '{snp}' due to mismatched lengths.\")\n","            continue\n","\n","        # Train the Random Forest model\n","        rf_model = RandomForestRegressor(n_estimators=50, random_state=42)\n","        rf_model.fit(X_snp_scaled, phenotype_chunk)\n","\n","        # Calculate the performance score (MSE in this case)\n","        predictions = rf_model.predict(X_snp_scaled)\n","        mse = mean_squared_error(phenotype_chunk, predictions)\n","\n","        # Append data only if successfully processed\n","        performance_scores.append(mse)\n","        chromosomes.append(chrom)\n","        positions.append(pos)\n","        processed_snps.append(snp)  # Track the SNP name\n","\n","    # Update processed rows count after each chunk\n","    processed_rows += len(chunk)\n","\n","# Step 4: Create DataFrame with successfully processed SNP names, performance scores, chromosomes, and positions\n","results_df = pd.DataFrame({\n","    'SNP': processed_snps,\n","    'performance_score': performance_scores,\n","    '-log10(Performance)': -np.log10(performance_scores),\n","    'Chromosome': chromosomes,\n","    'Position': positions\n","})\n","\n","# Sort by Chromosome and Position\n","results_df = results_df.sort_values(['Chromosome', 'Position'])\n","\n","# Select the top 200 SNPs by performance\n","top_200_snps = results_df.nlargest(200, '-log10(Performance)')\n","\n","# Save SNPs, chromosome, position, MSE, and -log10(Performance) values to a CSV file\n","top_200_snps[['SNP', 'Chromosome', 'Position', 'performance_score', '-log10(Performance)']].to_csv(\n","    \"top_200_snp_with_mse_and_log_performance.csv\", index=False\n",")\n","\n","# Set up figure size and axis for Manhattan plot\n","plt.figure(figsize=(12, 6))\n","\n","# Create 'chromosome position' to spread chromosomes across the x-axis\n","top_200_snps['Chromosome_Position'] = top_200_snps.groupby('Chromosome').cumcount() + 1\n","\n","# Create a color map to assign a different color to each chromosome\n","colors = plt.cm.get_cmap('tab20', 20)\n","\n","# Plot data for each chromosome separately to apply different colors\n","x_labels = []\n","x_ticks = []\n","cumulative_position = 0\n","\n","for chrom in top_200_snps['Chromosome'].unique():\n","    chrom_data = top_200_snps[top_200_snps['Chromosome'] == chrom]\n","    plt.scatter(chrom_data['Chromosome_Position'] + cumulative_position,\n","                chrom_data['-log10(Performance)'],\n","                color=colors(chrom - 1), label=f'Chr {chrom}', s=10, edgecolor='k')\n","\n","    # Add mid-point of each chromosome for labeling\n","    x_labels.append(f'Chr {chrom}')\n","    x_ticks.append(cumulative_position + (len(chrom_data) // 2))\n","\n","    # Update cumulative position\n","    cumulative_position += len(chrom_data)\n","\n","# Customize the plot without chromosome and position labels\n","plt.title('Manhattan Plot for Top 200 SNPs Based on Performance')\n","plt.xlabel('Chromosome')\n","plt.ylabel('-log10(Performance)')\n","plt.grid(True)\n","\n","# Add x-ticks at the midpoints of each chromosome's SNP positions\n","plt.xticks(ticks=x_ticks, labels=x_labels, rotation=45, ha='left')\n","\n","# Save the plot as a PNG image\n","plt.tight_layout()\n","plt.savefig('top_200_manhattan_plot.png', dpi=300)  # Save with high resolution (300 DPI)\n","\n","# Show the plot\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"PR3_env1","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}